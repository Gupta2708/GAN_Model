# Learning Probability Density Function using GAN

## ğŸ“Œ Project Overview

This project implements a **Generative Adversarial Network (GAN)** to learn the probability density function (PDF) of a transformed NOâ‚‚ concentration variable.

Instead of assuming a known analytical distribution, the GAN learns the distribution directly from real data samples.

Dataset: India Air Quality Dataset (Kaggle)  
Feature Used: NOâ‚‚ concentration  
Total Samples: 263,627 valid measurements  

---

## ğŸ”¢ Roll Number Parameterization

For roll number **102316130**, each NOâ‚‚ value `x` is transformed as:

z = x + aáµ£ sin(báµ£ x)

Where:

- aáµ£ = 0.5 Ã— (r mod 7) = **1.0**
- báµ£ = 0.3 Ã— ((r mod 5) + 1) = **0.3**

This introduces controlled non-linearity into the dataset.

The transformed data is normalized before training:

z_norm = (z âˆ’ mean) / std

Normalization ensures stable GAN training.

---

## ğŸ§  GAN Architecture

### Generator
- Input: 1D Gaussian noise
- Layers: Linear(1â†’32) â†’ ReLU â†’ Linear(32â†’32) â†’ ReLU â†’ Linear(32â†’1)
- Output: Synthetic sample from learned distribution

### Discriminator
- Input: 1D real or fake sample
- Layers: Linear(1â†’32) â†’ LeakyReLU â†’ Linear(32â†’32) â†’ LeakyReLU â†’ Linear(32â†’1) â†’ Sigmoid
- Output: Probability (real vs fake)

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|------------|--------|
| Epochs | 4000 |
| Batch Size | 128 |
| Optimizer | Adam |
| Learning Rate | 0.0002 |
| Loss Function | Binary Cross Entropy |
| Device | CPU/GPU |

Training alternates between:
- Updating Discriminator (real vs fake classification)
- Updating Generator (fooling the discriminator)

---

## ğŸ“Š PDF Approximation

After training:

- 10,000 samples are generated from the Generator
- Samples are denormalized
- Distribution comparison performed using:
  - Histogram overlay
  - Kernel Density Estimation (KDE)

---

## ğŸ“ˆ Results

### 1ï¸âƒ£ Histogram Comparison
- Real transformed data vs GAN-generated samples
- Strong visual similarity
- Major distribution modes captured
![Histogram Comparison](images/histogram.png)


### 2ï¸âƒ£ KDE-Based PDF Estimation
- Smooth continuous PDF curve
- Accurate approximation of empirical distribution
- No prior analytical form required
![KDE Curve](images/kde_curve.png)

---

## ğŸ” Key Observations

- GAN successfully learned complex transformed NOâ‚‚ distribution
- Training remained stable across 4000 epochs
- Simple architecture (2 hidden layers) was sufficient
- Data normalization was critical for convergence
- Generated distribution closely matches empirical PDF

---

## âœ… Conclusion

This project demonstrates that **GANs can effectively learn probability density functions directly from sample data**, without assuming any known distribution.

The approach is especially useful when:

- The analytical PDF is unknown
- The distribution is complex or multimodal
- Traditional parametric modeling is difficult

GAN-based distribution learning provides a powerful generative modeling framework for real-world environmental datasets.

---
